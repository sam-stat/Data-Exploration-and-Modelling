{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill na values of horsehorsepower and acceleration with average of the respective columns\n",
    "car_df=pd.read_csv(\"C:/Users/Mainak Kundu/Desktop/Samiran/Rakuten/data.csv\")\n",
    "car_df[\"horsepower\"].fillna((car_df[\"horsepower\"].mean()), inplace=True)\n",
    "car_df[\"acceleration\"].fillna((car_df[\"acceleration\"].mean()), inplace=True)\n",
    "\n",
    "#removing extra large values of cylinders and model year\n",
    "car_df = car_df.drop(car_df[car_df[\"cylinders\"] >8 ].index)\n",
    "car_df = car_df.drop(car_df[car_df[\"model year\"] > 82].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is applied on the data after removing the \"car name\" feature and removing anomalies for cylinder and model year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8250051486909883 and Test accuracy: 0.8112807219286456\n"
     ]
    }
   ],
   "source": [
    "#We are removing car name column from the data set for our initial analysis\n",
    "input_df=car_df.drop(\"car name\",axis=1)\n",
    "\n",
    "#building a linear regression model \n",
    "linear_model = LinearRegression()\n",
    "X =input_df.loc[:, input_df.columns != \"mpg\"]\n",
    "y =input_df[\"mpg\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "lm_fit=linear_model.fit(x_train,y_train)\n",
    "r2_test = r2_score(y_test,lm_fit.predict(x_test))\n",
    "r2_train = r2_score(y_train,lm_fit.predict(x_train))\n",
    "\n",
    "#final r2 scores\n",
    "print(\"Train accuracy: {} and Test accuracy: {}\".format(r2_train,r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression tree regression is applied on the data after removing the \"car name\" feature and removing anomalies for cylinder and model year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0 and Test accuracy: 0.6669754519741429\n"
     ]
    }
   ],
   "source": [
    "#We are removing car name column from the data set for our initial analysis\n",
    "input_df=car_df.drop(\"car name\",axis=1)\n",
    "\n",
    "#list of all variables for which one-hot-encoding has to be done\n",
    "discrete=[\"cylinders\",\"model year\",\"origin\"]\n",
    "\n",
    "#One-hot encoding of all the categorical features\n",
    "input_df=pd.get_dummies(data=input_df, columns=discrete)\n",
    "\n",
    "#building a tree based model\n",
    "tree_model = DecisionTreeRegressor(random_state=0,criterion=\"mae\")\n",
    "X =input_df.loc[:, input_df.columns != \"mpg\"]\n",
    "y =input_df[\"mpg\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "dt_fit=tree_model.fit(x_train, y_train)\n",
    "r2_test = r2_score(y_test,dt_fit.predict(x_test))\n",
    "r2_train = r2_score(y_train,dt_fit.predict(x_train))\n",
    "\n",
    "#final r2 scores\n",
    "print(\"Train accuracy: {} and Test accuracy: {}\".format(r2_train,r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: Decision tree is overfitting the data too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XgBoost model with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are removing car name column from the data set for our initial analysis\n",
    "input_df=car_df.drop(\"car name\",axis=1)\n",
    "\n",
    "#list of all variables for which one-hot-encoding has to be done\n",
    "discrete=[\"cylinders\",\"model year\",\"origin\"]\n",
    "\n",
    "#One-hot encoding of all the categorical features\n",
    "input_df=pd.get_dummies(data=input_df, columns=discrete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into full hyperparameter turing for xgboost we will try to fit a basic xg boost tree on the data \n",
    "and then apply cross validation to get better estimates of the hyparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mainak Kundu\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9999999863569797 and Test accuracy: 0.8369761540733889\n"
     ]
    }
   ],
   "source": [
    "#xg boost model with default values\n",
    "xgb = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "#dividing the data in train test\n",
    "X =input_df.loc[:, input_df.columns != \"mpg\"]\n",
    "y =input_df[\"mpg\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "#default xgb model\n",
    "xgb_model=xgb.fit(x_train, y_train)\n",
    "\n",
    "r2_test = r2_score(y_test,xgb_model.predict(x_test))\n",
    "r2_train = r2_score(y_train,xgb_model.predict(x_train))\n",
    "\n",
    "#final r2 scores\n",
    "print(\"Train accuracy: {} and Test accuracy: {}\".format(r2_train,r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we found out that the model is overfitting the data too much. We will try to improve this one by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter of the xgboost, that is learning rate will be fixed which\n",
    "we search of other sets of parameter. Onces all the set of paramets are already \n",
    "obtained we will go into finding the most optimum value of the learning parameter.\n",
    " \n",
    "If we consider all the parametrs at onces then it will be stuck as the combinations\n",
    "of all the parameters will be huge to handle with. So we will apply a greedy approach\n",
    "where we will try to optimize the hyperparameter taking two at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Max depth and min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mainak Kundu\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 5}, 0.8486143303431414)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=5,\n",
    "min_child_weight=1,    \n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test1,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch1.fit(x_train,y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'min_child_weight': 4}, 0.8533383222065772)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':range(2,5,1),\n",
    " 'min_child_weight':range(3,7,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=5,\n",
    "min_child_weight=1,    \n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test2,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch1.fit(x_train,y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best parameteres for max depth is=3 and min_child_weight =4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.8533383222065772)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=3,\n",
    "min_child_weight=4,    \n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test3,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch3.fit(x_train,y_train)\n",
    "gsearch3.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.8533383222065772)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'gamma':[i/100.0 for i in range(0,30,2)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=3,\n",
    "min_child_weight=4,    \n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test4,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch4.fit(x_train,y_train)\n",
    "gsearch4.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the optimum value of gamma is 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.8, 'subsample': 0.9}, 0.8533383222065772)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=3,\n",
    "min_child_weight=4,    \n",
    "gamma=0.00,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test5,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch5.fit(x_train,y_train)\n",
    "gsearch5.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.95, 'subsample': 0.85}, 0.8533383222065772)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'subsample':[i/100.0 for i in range(70,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(70,100,5)]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=3,\n",
    "min_child_weight=4,    \n",
    "gamma=0.0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'reg:linear',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test6,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch6.fit(x_train,y_train)\n",
    "gsearch6.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best values of colsample_bytree is 0.85 and subsample is 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tuning reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 0.01}, 0.8589803166740173)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch7 =GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=3,\n",
    "min_child_weight=4,    \n",
    "gamma=0.00,\n",
    "subsample=0.85,\n",
    "colsample_bytree=0.95,\n",
    "objective= 'reg:linear',\n",
    "reg_alpha=0.1,\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test7,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch7.fit(x_train,y_train)\n",
    "gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 0.1}, 0.8294566807384797)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test8= {\n",
    " 'reg_alpha':[0.005,0.01,0.015,0.02]\n",
    "}\n",
    "gsearch8 =GridSearchCV(estimator = XGBRegressor( \n",
    "\n",
    "learning_rate =0.1,\n",
    "n_estimators=140,\n",
    "max_depth=4,\n",
    "min_child_weight=1,    \n",
    "gamma=0.0,\n",
    "subsample=0.85,\n",
    "colsample_bytree=0.95,\n",
    "objective= 'reg:linear',\n",
    "reg_alpha=0.1,\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27), \n",
    " param_grid = param_test7,\n",
    "n_jobs=4,\n",
    "iid=False,\n",
    "cv=5)\n",
    "gsearch8.fit(x_train,y_train)\n",
    "gsearch8.best_params_, gsearch8.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tuning learning rate: Now we will reduce the learning rate and find the optimum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.957182934176924 and Test accuracy: 0.8572520306269685\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBRegressor(\n",
    " learning_rate =0.01,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=4,\n",
    " gamma=0.00,\n",
    " subsample=0.85,\n",
    " reg_alpha=0.1,\n",
    " colsample_bytree=0.95,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "#dividing the data in train test\n",
    "X =input_df.loc[:, input_df.columns != \"mpg\"]\n",
    "y =input_df[\"mpg\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "#default xgb model\n",
    "xgb_model=xgb.fit(x_train, y_train)\n",
    "\n",
    "r2_test = r2_score(y_test,xgb_model.predict(x_test))\n",
    "r2_train = r2_score(y_train,xgb_model.predict(x_train))\n",
    "\n",
    "#final r2 scores\n",
    "print(\"Train accuracy: {} and Test accuracy: {}\".format(r2_train,r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we found out the model which is better than the original model proposed by 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
